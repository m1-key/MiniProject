{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing,\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./train.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.columns = ['deceptive','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceptive</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   deceptive                                               text\n",
       "0          1  Unfortunately, the frustration of being Dr. Go...\n",
       "1          2  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2          1  I don't know what Dr. Goldberg was like before...\n",
       "3          1  I'm writing this review to give you a heads up...\n",
       "4          2  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('./test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.columns = ['deceptive','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceptive</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   deceptive                                               text\n",
       "0          2  Contrary to other reviews, I have zero complai...\n",
       "1          1  Last summer I had an appointment to get new ti...\n",
       "2          2  Friendly staff, same starbucks fair you get an...\n",
       "3          1  The food is good. Unfortunately the service is...\n",
       "4          2  Even when we didn't have a car Filene's Baseme..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 560000 entries, 0 to 559999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   deceptive  560000 non-null  int64 \n",
      " 1   text       560000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m1key/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of Deceptive and Non Deceptive reviews (Deceptive=1 & NonDeceptive=2)')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data_train.deceptive)\n",
    "plt.xlabel('Deceptive')\n",
    "plt.title('Number of Deceptive and Non Deceptive reviews (Deceptive=1 & NonDeceptive=2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deceptive</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>280000</td>\n",
       "      <td>280000</td>\n",
       "      <td>The bathrooms were abysmal in the early aftern...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>280000</td>\n",
       "      <td>280000</td>\n",
       "      <td>Thrifty's Ice Cream Shoppe is some of the best...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             text                                                             \\\n",
       "            count  unique                                                top   \n",
       "deceptive                                                                      \n",
       "1          280000  280000  The bathrooms were abysmal in the early aftern...   \n",
       "2          280000  280000  Thrifty's Ice Cream Shoppe is some of the best...   \n",
       "\n",
       "                \n",
       "          freq  \n",
       "deceptive       \n",
       "1            1  \n",
       "2            1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset description\n",
    "data_train.groupby('deceptive').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  word_count\n",
       "0  Unfortunately, the frustration of being Dr. Go...         122\n",
       "1  Been going to Dr. Goldberg for over 10 years. ...          97\n",
       "2  I don't know what Dr. Goldberg was like before...         212\n",
       "3  I'm writing this review to give you a heads up...         193\n",
       "4  All the food is great here. But the best thing...          80"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word count\n",
    "data_train['word_count'] = data_train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "data_train[['text','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "      <td>1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  char_count\n",
       "0  Unfortunately, the frustration of being Dr. Go...         643\n",
       "1  Been going to Dr. Goldberg for over 10 years. ...         495\n",
       "2  I don't know what Dr. Goldberg was like before...        1143\n",
       "3  I'm writing this review to give you a heads up...        1050\n",
       "4  All the food is great here. But the best thing...         425"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#character count including spaces\n",
    "data_train['char_count'] = data_train['text'].str.len() ## this also includes spaces\n",
    "data_train[['text','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>4.539130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>4.113402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "      <td>4.417062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "      <td>4.445596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "      <td>4.613333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  avg_word\n",
       "0  Unfortunately, the frustration of being Dr. Go...  4.539130\n",
       "1  Been going to Dr. Goldberg for over 10 years. ...  4.113402\n",
       "2  I don't know what Dr. Goldberg was like before...  4.417062\n",
       "3  I'm writing this review to give you a heads up...  4.445596\n",
       "4  All the food is great here. But the best thing...  4.613333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average word length\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "data_train['avg_word'] = data_train['text'].apply(lambda x: avg_word(x))\n",
    "data_train[['text','avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stopwords\n",
       "0  Unfortunately, the frustration of being Dr. Go...         47\n",
       "1  Been going to Dr. Goldberg for over 10 years. ...         47\n",
       "2  I don't know what Dr. Goldberg was like before...         96\n",
       "3  I'm writing this review to give you a heads up...         79\n",
       "4  All the food is great here. But the best thing...         21"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "data_train['stopwords'] = data_train['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "data_train[['text','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spchar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spchar\n",
       "0  Unfortunately, the frustration of being Dr. Go...       0\n",
       "1  Been going to Dr. Goldberg for over 10 years. ...       0\n",
       "2  I don't know what Dr. Goldberg was like before...       0\n",
       "3  I'm writing this review to give you a heads up...       0\n",
       "4  All the food is great here. But the best thing...       0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no of special characters\n",
    "data_train['spchar'] = data_train['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "data_train[['text','spchar']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  numerics\n",
       "0  Unfortunately, the frustration of being Dr. Go...         2\n",
       "1  Been going to Dr. Goldberg for over 10 years. ...         1\n",
       "2  I don't know what Dr. Goldberg was like before...         1\n",
       "3  I'm writing this review to give you a heads up...         0\n",
       "4  All the food is great here. But the best thing...         0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no of numerics\n",
    "data_train['numerics'] = data_train['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "data_train[['text','numerics']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  upper\n",
       "0  Unfortunately, the frustration of being Dr. Go...      5\n",
       "1  Been going to Dr. Goldberg for over 10 years. ...      5\n",
       "2  I don't know what Dr. Goldberg was like before...      8\n",
       "3  I'm writing this review to give you a heads up...     10\n",
       "4  All the food is great here. But the best thing...      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no of uppercase characters\n",
    "data_train['upper'] = data_train['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "data_train[['text','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    unfortunately, the frustration of being dr. go...\n",
       "1    been going to dr. goldberg for over 10 years. ...\n",
       "2    i don't know what dr. goldberg was like before...\n",
       "3    i'm writing this review to give you a heads up...\n",
       "4    all the food is great here. but the best thing...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to lowercase\n",
    "data_train['text'] = data_train['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data_train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-fe756c47003f>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_train['text'] = data_train['text'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    unfortunately the frustration of being dr gold...\n",
       "1    been going to dr goldberg for over 10 years i ...\n",
       "2    i dont know what dr goldberg was like before m...\n",
       "3    im writing this review to give you a heads up ...\n",
       "4    all the food is great here but the best thing ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing punctuation\n",
    "data_train['text'] = data_train['text'].str.replace('[^\\w\\s]','')\n",
    "data_train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    unfortunately frustration dr goldbergs patient...\n",
       "1    going dr goldberg 10 years think one 1st patie...\n",
       "2    dont know dr goldberg like moving arizona let ...\n",
       "3    im writing review give heads see doctor office...\n",
       "4    food great best thing wings wings simply fanta...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data_train['text'] = data_train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data_train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "food       319953\n",
       "place      316445\n",
       "good       290108\n",
       "like       259145\n",
       "get        235863\n",
       "one        231827\n",
       "time       209760\n",
       "would      206953\n",
       "great      205939\n",
       "service    202014\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing common word\n",
    "freq = pd.Series(' '.join(data_train['text']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    unfortunately frustration dr goldbergs patient...\n",
       "1    going dr goldberg 10 years think 1st patients ...\n",
       "2    dont know dr goldberg moving arizona let tell ...\n",
       "3    im writing review give heads see doctor office...\n",
       "4    best thing wings wings simply fantastic wet ca...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing common word\n",
    "freq = list(freq.index)\n",
    "data_train['text'] = data_train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data_train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nightncasual          1\n",
       "knowsits              1\n",
       "perfectlynnso         1\n",
       "clientelenngroupon    1\n",
       "yearnnsonic           1\n",
       "streetnalong          1\n",
       "pu00f9re              1\n",
       "casualthe             1\n",
       "onlinenwe             1\n",
       "etcnoverall           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remvoing rare words\n",
    "freq = pd.Series(' '.join(data_train['text']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    unfortunately frustration dr goldbergs patient...\n",
       "1    going dr goldberg 10 years think 1st patients ...\n",
       "2    dont know dr goldberg moving arizona let tell ...\n",
       "3    im writing review give heads see doctor office...\n",
       "4    best thing wings wings simply fantastic wet ca...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing rare words\n",
    "freq = list(freq.index)\n",
    "data_train['text'] = data_train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data_train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    unfortunately frustration dr goldbergs patient...\n",
       "1    going dr goldberg 10 years think st patients s...\n",
       "2    dont know dr goldberg moving arizona let tell ...\n",
       "3    in writing review give heads see doctor office...\n",
       "4    best thing wings wings simply fantastic wet ca...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spelling correction\n",
    "from textblob import TextBlob\n",
    "data_train['text'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/m1key/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['going', 'dr', 'goldberg', '10', 'years', 'think', '1st', 'patients', 'started', 'mhmg', 'hes', 'years', 'really', 'big', 'picture', 'former', 'gyn', 'dr', 'markoff', 'found', 'fibroids', 'explores', 'options', 'patient', 'understanding', 'doesnt', 'judge', 'asks', 'right', 'questions', 'thorough', 'wants', 'kept', 'loop', 'every', 'aspect', 'medical', 'health', 'life'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "TextBlob(data_train['text'][1]).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    unfortun frustrat dr goldberg patient repeat e...\n",
       "1    go dr goldberg 10 year think 1st patient start...\n",
       "2    dont know dr goldberg move arizona let tell st...\n",
       "3    im write review give head see doctor offic sta...\n",
       "4    best thing wing wing simpli fantast wet cajun ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "data_train['text'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/m1key/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    unfortunately frustration dr goldberg patient ...\n",
       "1    going dr goldberg 10 year think 1st patient st...\n",
       "2    dont know dr goldberg moving arizona let tell ...\n",
       "3    im writing review give head see doctor office ...\n",
       "4    best thing wing wing simply fantastic wet caju...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmetization\n",
    "from textblob import Word\n",
    "data_train['text'] = data_train['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data_train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advance Text Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['unfortunately', 'frustration']),\n",
       " WordList(['frustration', 'dr']),\n",
       " WordList(['dr', 'goldberg']),\n",
       " WordList(['goldberg', 'patient']),\n",
       " WordList(['patient', 'repeat']),\n",
       " WordList(['repeat', 'experience']),\n",
       " WordList(['experience', 'ive']),\n",
       " WordList(['ive', 'many']),\n",
       " WordList(['many', 'doctor']),\n",
       " WordList(['doctor', 'nyc']),\n",
       " WordList(['nyc', 'doctor']),\n",
       " WordList(['doctor', 'terrible']),\n",
       " WordList(['terrible', 'staff']),\n",
       " WordList(['staff', 'seems']),\n",
       " WordList(['seems', 'staff']),\n",
       " WordList(['staff', 'simply']),\n",
       " WordList(['simply', 'never']),\n",
       " WordList(['never', 'answer']),\n",
       " WordList(['answer', 'phone']),\n",
       " WordList(['phone', 'usually']),\n",
       " WordList(['usually', 'take']),\n",
       " WordList(['take', '2']),\n",
       " WordList(['2', 'hour']),\n",
       " WordList(['hour', 'repeated']),\n",
       " WordList(['repeated', 'calling']),\n",
       " WordList(['calling', 'answer']),\n",
       " WordList(['answer', 'want']),\n",
       " WordList(['want', 'deal']),\n",
       " WordList(['deal', 'run']),\n",
       " WordList(['run', 'problem']),\n",
       " WordList(['problem', 'many']),\n",
       " WordList(['many', 'doctor']),\n",
       " WordList(['doctor', 'dont']),\n",
       " WordList(['dont', 'office']),\n",
       " WordList(['office', 'worker']),\n",
       " WordList(['worker', 'patient']),\n",
       " WordList(['patient', 'medical']),\n",
       " WordList(['medical', 'need']),\n",
       " WordList(['need', 'isnt']),\n",
       " WordList(['isnt', 'anyone']),\n",
       " WordList(['anyone', 'answering']),\n",
       " WordList(['answering', 'phone']),\n",
       " WordList(['phone', 'incomprehensible']),\n",
       " WordList(['incomprehensible', 'work']),\n",
       " WordList(['work', 'aggravation']),\n",
       " WordList(['aggravation', 'regret']),\n",
       " WordList(['regret', 'feel']),\n",
       " WordList(['feel', 'give']),\n",
       " WordList(['give', 'dr']),\n",
       " WordList(['dr', 'goldberg']),\n",
       " WordList(['goldberg', '2']),\n",
       " WordList(['2', 'star'])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#N-grams\n",
    "TextBlob(data_train['text'][0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patient</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dr</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>year</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>former</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aspect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>option</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>he</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>going</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>markoff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>right</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>think</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doesnt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>big</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>understanding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>explores</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kept</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gyn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>picture</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fibroid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>medical</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>thorough</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>found</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>judge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>really</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>asks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>every</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mhmg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>life</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>goldberg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1st</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>loop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>health</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            words  tf\n",
       "0         patient   2\n",
       "1              dr   2\n",
       "2            year   2\n",
       "3          former   1\n",
       "4          aspect   1\n",
       "5          option   1\n",
       "6              he   1\n",
       "7        question   1\n",
       "8           going   1\n",
       "9         markoff   1\n",
       "10          right   1\n",
       "11          think   1\n",
       "12         doesnt   1\n",
       "13            big   1\n",
       "14  understanding   1\n",
       "15       explores   1\n",
       "16           kept   1\n",
       "17            gyn   1\n",
       "18        picture   1\n",
       "19        fibroid   1\n",
       "20        medical   1\n",
       "21       thorough   1\n",
       "22             10   1\n",
       "23          found   1\n",
       "24          judge   1\n",
       "25         really   1\n",
       "26           asks   1\n",
       "27          every   1\n",
       "28           mhmg   1\n",
       "29           life   1\n",
       "30       goldberg   1\n",
       "31            1st   1\n",
       "32           want   1\n",
       "33        started   1\n",
       "34           loop   1\n",
       "35         health   1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Term frequency\n",
    "tf1 = (data_train['text'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patient</td>\n",
       "      <td>2</td>\n",
       "      <td>4.511322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dr</td>\n",
       "      <td>2</td>\n",
       "      <td>1.218428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>year</td>\n",
       "      <td>2</td>\n",
       "      <td>2.425459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>former</td>\n",
       "      <td>1</td>\n",
       "      <td>5.205283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aspect</td>\n",
       "      <td>1</td>\n",
       "      <td>5.871779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>option</td>\n",
       "      <td>1</td>\n",
       "      <td>3.140345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>he</td>\n",
       "      <td>1</td>\n",
       "      <td>0.365561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "      <td>3.767768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>going</td>\n",
       "      <td>1</td>\n",
       "      <td>2.029686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>markoff</td>\n",
       "      <td>1</td>\n",
       "      <td>12.137080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>right</td>\n",
       "      <td>1</td>\n",
       "      <td>2.083736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>think</td>\n",
       "      <td>1</td>\n",
       "      <td>2.073815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doesnt</td>\n",
       "      <td>1</td>\n",
       "      <td>3.131593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>big</td>\n",
       "      <td>1</td>\n",
       "      <td>2.447301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>understanding</td>\n",
       "      <td>1</td>\n",
       "      <td>5.768893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>explores</td>\n",
       "      <td>1</td>\n",
       "      <td>11.289782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kept</td>\n",
       "      <td>1</td>\n",
       "      <td>3.571541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gyn</td>\n",
       "      <td>1</td>\n",
       "      <td>6.782067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>picture</td>\n",
       "      <td>1</td>\n",
       "      <td>4.116809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fibroid</td>\n",
       "      <td>1</td>\n",
       "      <td>11.626254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>medical</td>\n",
       "      <td>1</td>\n",
       "      <td>6.007304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>thorough</td>\n",
       "      <td>1</td>\n",
       "      <td>5.026928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2.262695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>found</td>\n",
       "      <td>1</td>\n",
       "      <td>2.801429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>judge</td>\n",
       "      <td>1</td>\n",
       "      <td>5.420889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>really</td>\n",
       "      <td>1</td>\n",
       "      <td>1.534670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>asks</td>\n",
       "      <td>1</td>\n",
       "      <td>5.582197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>every</td>\n",
       "      <td>1</td>\n",
       "      <td>1.557829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mhmg</td>\n",
       "      <td>1</td>\n",
       "      <td>13.235692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>life</td>\n",
       "      <td>1</td>\n",
       "      <td>3.485531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>goldberg</td>\n",
       "      <td>1</td>\n",
       "      <td>11.038467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1st</td>\n",
       "      <td>1</td>\n",
       "      <td>4.925031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "      <td>1.706942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "      <td>3.265013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>loop</td>\n",
       "      <td>1</td>\n",
       "      <td>6.802752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>health</td>\n",
       "      <td>1</td>\n",
       "      <td>4.004471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            words  tf        idf\n",
       "0         patient   2   4.511322\n",
       "1              dr   2   1.218428\n",
       "2            year   2   2.425459\n",
       "3          former   1   5.205283\n",
       "4          aspect   1   5.871779\n",
       "5          option   1   3.140345\n",
       "6              he   1   0.365561\n",
       "7        question   1   3.767768\n",
       "8           going   1   2.029686\n",
       "9         markoff   1  12.137080\n",
       "10          right   1   2.083736\n",
       "11          think   1   2.073815\n",
       "12         doesnt   1   3.131593\n",
       "13            big   1   2.447301\n",
       "14  understanding   1   5.768893\n",
       "15       explores   1  11.289782\n",
       "16           kept   1   3.571541\n",
       "17            gyn   1   6.782067\n",
       "18        picture   1   4.116809\n",
       "19        fibroid   1  11.626254\n",
       "20        medical   1   6.007304\n",
       "21       thorough   1   5.026928\n",
       "22             10   1   2.262695\n",
       "23          found   1   2.801429\n",
       "24          judge   1   5.420889\n",
       "25         really   1   1.534670\n",
       "26           asks   1   5.582197\n",
       "27          every   1   1.557829\n",
       "28           mhmg   1  13.235692\n",
       "29           life   1   3.485531\n",
       "30       goldberg   1  11.038467\n",
       "31            1st   1   4.925031\n",
       "32           want   1   1.706942\n",
       "33        started   1   3.265013\n",
       "34           loop   1   6.802752\n",
       "35         health   1   4.004471"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inverse document frequency\n",
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(data_train.shape[0]/(len(data_train[data_train['text'].str.contains(word)])))\n",
    "\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patient</td>\n",
       "      <td>2</td>\n",
       "      <td>4.511322</td>\n",
       "      <td>9.022644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dr</td>\n",
       "      <td>2</td>\n",
       "      <td>1.218428</td>\n",
       "      <td>2.436856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>year</td>\n",
       "      <td>2</td>\n",
       "      <td>2.425459</td>\n",
       "      <td>4.850918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>former</td>\n",
       "      <td>1</td>\n",
       "      <td>5.205283</td>\n",
       "      <td>5.205283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aspect</td>\n",
       "      <td>1</td>\n",
       "      <td>5.871779</td>\n",
       "      <td>5.871779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>option</td>\n",
       "      <td>1</td>\n",
       "      <td>3.140345</td>\n",
       "      <td>3.140345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>he</td>\n",
       "      <td>1</td>\n",
       "      <td>0.365561</td>\n",
       "      <td>0.365561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "      <td>3.767768</td>\n",
       "      <td>3.767768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>going</td>\n",
       "      <td>1</td>\n",
       "      <td>2.029686</td>\n",
       "      <td>2.029686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>markoff</td>\n",
       "      <td>1</td>\n",
       "      <td>12.137080</td>\n",
       "      <td>12.137080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>right</td>\n",
       "      <td>1</td>\n",
       "      <td>2.083736</td>\n",
       "      <td>2.083736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>think</td>\n",
       "      <td>1</td>\n",
       "      <td>2.073815</td>\n",
       "      <td>2.073815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doesnt</td>\n",
       "      <td>1</td>\n",
       "      <td>3.131593</td>\n",
       "      <td>3.131593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>big</td>\n",
       "      <td>1</td>\n",
       "      <td>2.447301</td>\n",
       "      <td>2.447301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>understanding</td>\n",
       "      <td>1</td>\n",
       "      <td>5.768893</td>\n",
       "      <td>5.768893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>explores</td>\n",
       "      <td>1</td>\n",
       "      <td>11.289782</td>\n",
       "      <td>11.289782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kept</td>\n",
       "      <td>1</td>\n",
       "      <td>3.571541</td>\n",
       "      <td>3.571541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gyn</td>\n",
       "      <td>1</td>\n",
       "      <td>6.782067</td>\n",
       "      <td>6.782067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>picture</td>\n",
       "      <td>1</td>\n",
       "      <td>4.116809</td>\n",
       "      <td>4.116809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fibroid</td>\n",
       "      <td>1</td>\n",
       "      <td>11.626254</td>\n",
       "      <td>11.626254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>medical</td>\n",
       "      <td>1</td>\n",
       "      <td>6.007304</td>\n",
       "      <td>6.007304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>thorough</td>\n",
       "      <td>1</td>\n",
       "      <td>5.026928</td>\n",
       "      <td>5.026928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2.262695</td>\n",
       "      <td>2.262695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>found</td>\n",
       "      <td>1</td>\n",
       "      <td>2.801429</td>\n",
       "      <td>2.801429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>judge</td>\n",
       "      <td>1</td>\n",
       "      <td>5.420889</td>\n",
       "      <td>5.420889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>really</td>\n",
       "      <td>1</td>\n",
       "      <td>1.534670</td>\n",
       "      <td>1.534670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>asks</td>\n",
       "      <td>1</td>\n",
       "      <td>5.582197</td>\n",
       "      <td>5.582197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>every</td>\n",
       "      <td>1</td>\n",
       "      <td>1.557829</td>\n",
       "      <td>1.557829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mhmg</td>\n",
       "      <td>1</td>\n",
       "      <td>13.235692</td>\n",
       "      <td>13.235692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>life</td>\n",
       "      <td>1</td>\n",
       "      <td>3.485531</td>\n",
       "      <td>3.485531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>goldberg</td>\n",
       "      <td>1</td>\n",
       "      <td>11.038467</td>\n",
       "      <td>11.038467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1st</td>\n",
       "      <td>1</td>\n",
       "      <td>4.925031</td>\n",
       "      <td>4.925031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "      <td>1.706942</td>\n",
       "      <td>1.706942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>started</td>\n",
       "      <td>1</td>\n",
       "      <td>3.265013</td>\n",
       "      <td>3.265013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>loop</td>\n",
       "      <td>1</td>\n",
       "      <td>6.802752</td>\n",
       "      <td>6.802752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>health</td>\n",
       "      <td>1</td>\n",
       "      <td>4.004471</td>\n",
       "      <td>4.004471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            words  tf        idf      tfidf\n",
       "0         patient   2   4.511322   9.022644\n",
       "1              dr   2   1.218428   2.436856\n",
       "2            year   2   2.425459   4.850918\n",
       "3          former   1   5.205283   5.205283\n",
       "4          aspect   1   5.871779   5.871779\n",
       "5          option   1   3.140345   3.140345\n",
       "6              he   1   0.365561   0.365561\n",
       "7        question   1   3.767768   3.767768\n",
       "8           going   1   2.029686   2.029686\n",
       "9         markoff   1  12.137080  12.137080\n",
       "10          right   1   2.083736   2.083736\n",
       "11          think   1   2.073815   2.073815\n",
       "12         doesnt   1   3.131593   3.131593\n",
       "13            big   1   2.447301   2.447301\n",
       "14  understanding   1   5.768893   5.768893\n",
       "15       explores   1  11.289782  11.289782\n",
       "16           kept   1   3.571541   3.571541\n",
       "17            gyn   1   6.782067   6.782067\n",
       "18        picture   1   4.116809   4.116809\n",
       "19        fibroid   1  11.626254  11.626254\n",
       "20        medical   1   6.007304   6.007304\n",
       "21       thorough   1   5.026928   5.026928\n",
       "22             10   1   2.262695   2.262695\n",
       "23          found   1   2.801429   2.801429\n",
       "24          judge   1   5.420889   5.420889\n",
       "25         really   1   1.534670   1.534670\n",
       "26           asks   1   5.582197   5.582197\n",
       "27          every   1   1.557829   1.557829\n",
       "28           mhmg   1  13.235692  13.235692\n",
       "29           life   1   3.485531   3.485531\n",
       "30       goldberg   1  11.038467  11.038467\n",
       "31            1st   1   4.925031   4.925031\n",
       "32           want   1   1.706942   1.706942\n",
       "33        started   1   3.265013   3.265013\n",
       "34           loop   1   6.802752   6.802752\n",
       "35         health   1   4.004471   4.004471"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#term freq - inverse document freq\n",
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<560000x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16479223 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sparse matrix tf-idf freq\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(data_train['text'])\n",
    "\n",
    "train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<560000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19312429 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(data_train['text'])\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceptive</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>spchar</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>unfortunately frustration dr goldberg patient ...</td>\n",
       "      <td>122</td>\n",
       "      <td>643</td>\n",
       "      <td>4.539130</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>going dr goldberg 10 year think 1st patient st...</td>\n",
       "      <td>97</td>\n",
       "      <td>495</td>\n",
       "      <td>4.113402</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>dont know dr goldberg moving arizona let tell ...</td>\n",
       "      <td>212</td>\n",
       "      <td>1143</td>\n",
       "      <td>4.417062</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>im writing review give head see doctor office ...</td>\n",
       "      <td>193</td>\n",
       "      <td>1050</td>\n",
       "      <td>4.445596</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>best thing wing wing simply fantastic wet caju...</td>\n",
       "      <td>80</td>\n",
       "      <td>425</td>\n",
       "      <td>4.613333</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>wing sauce water pretty much lot butter hot sa...</td>\n",
       "      <td>62</td>\n",
       "      <td>307</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>owning driving range inside city limit license...</td>\n",
       "      <td>234</td>\n",
       "      <td>1143</td>\n",
       "      <td>4.193548</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>absolute garbage half tee available including ...</td>\n",
       "      <td>115</td>\n",
       "      <td>563</td>\n",
       "      <td>4.157407</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>finally made range heard thing people fine go ...</td>\n",
       "      <td>187</td>\n",
       "      <td>939</td>\n",
       "      <td>4.026738</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>drove yesterday sneak peak reopens july 14th c...</td>\n",
       "      <td>78</td>\n",
       "      <td>370</td>\n",
       "      <td>4.013699</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   deceptive                                               text  word_count  \\\n",
       "0          1  unfortunately frustration dr goldberg patient ...         122   \n",
       "1          2  going dr goldberg 10 year think 1st patient st...          97   \n",
       "2          1  dont know dr goldberg moving arizona let tell ...         212   \n",
       "3          1  im writing review give head see doctor office ...         193   \n",
       "4          2  best thing wing wing simply fantastic wet caju...          80   \n",
       "5          1  wing sauce water pretty much lot butter hot sa...          62   \n",
       "6          1  owning driving range inside city limit license...         234   \n",
       "7          1  absolute garbage half tee available including ...         115   \n",
       "8          2  finally made range heard thing people fine go ...         187   \n",
       "9          2  drove yesterday sneak peak reopens july 14th c...          78   \n",
       "\n",
       "   char_count  avg_word  stopwords  spchar  numerics  upper  \n",
       "0         643  4.539130         47       0         2      5  \n",
       "1         495  4.113402         47       0         1      5  \n",
       "2        1143  4.417062         96       0         1      8  \n",
       "3        1050  4.445596         79       0         0     10  \n",
       "4         425  4.613333         21       0         0      1  \n",
       "5         307  4.100000         22       0         0      0  \n",
       "6        1143  4.193548         88       0         1      5  \n",
       "7         563  4.157407         50       0         1      1  \n",
       "8         939  4.026738         88       0         0      6  \n",
       "9         370  4.013699         23       0         0      2  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-830ed5e65d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_train['text'].astype(str)\n",
    "y = data_train['deceptive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100962    negative review slow play course time slow dow...\n",
       "502466    truffle fry win know feel hit jackpot vega try...\n",
       "272300    largest forever21 ive ever seennnthey much stu...\n",
       "543152    big u00e0 leur poutine glacu00e9e u00e9nooooor...\n",
       "559480    first experience midweek return visit disappoi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100962    1\n",
       "502466    2\n",
       "272300    2\n",
       "543152    2\n",
       "559480    1\n",
       "Name: deceptive, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52087     september 6 2013norder go roll ahead busy satu...\n",
       "212502    place really wanted love truly looking forward...\n",
       "115816                     way better excepted kid fun free\n",
       "330676    want guy badly always really creative cupcake ...\n",
       "400425    nice atmosphere interesting wine selection som...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52087     2\n",
       "212502    1\n",
       "115816    2\n",
       "330676    1\n",
       "400425    2\n",
       "Name: deceptive, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420000,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x_train)\n",
    "sequences = tok.texts_to_sequences(x_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x_test)\n",
    "sequences_test = tok.texts_to_sequences(x_test)\n",
    "sequences_matrix_test = sequence.pad_sequences(sequences_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=None,lower=True,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',split=' ',char_level=False)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1=tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 728528\n",
      "Longest comment size: 4481\n",
      "Average comment size: 423.33225238095235\n",
      "Stdev of comment size: 395.7559345207035\n",
      "Max comment size: 1610\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_index)\n",
    "print('Vocab size: {}'.format(vocab_size))\n",
    "longest = max(len(seq) for seq in x_train)\n",
    "print(\"Longest comment size: {}\".format(longest))\n",
    "average = np.mean([len(seq) for seq in x_train])\n",
    "print(\"Average comment size: {}\".format(average))\n",
    "stdev = np.std([len(seq) for seq in x_train])\n",
    "print(\"Stdev of comment size: {}\".format(stdev))\n",
    "max_len = int(average + stdev * 3)\n",
    "print('Max comment size: {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_x_train = pad_sequences(x_train1, maxlen=max_len, padding='post', truncating='post')\n",
    "processed_x_test = pad_sequences(x_test1, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop,Nadam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 1610)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1610, 50)          50000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 96,337\n",
      "Trainable params: 96,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(processed_x_train,y_train,batch_size=128,epochs=10,\n",
    "          validation_data=(processed_x_test,y_test),callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNGRU, Dense, Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout, GlobalMaxPooling1D, BatchNormalization, LSTM\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings ------- GloVe 100D ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('../input/glove-global-vectors-for-word-representation', 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "k = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        k += 1\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "#model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove.summary()\n",
    "model_glove.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove.fit(processed_x_train,y_train,batch_size=128,epochs=10,\n",
    "          validation_data=(processed_x_test,y_test),callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initate model\n",
    "model3 = Sequential()\n",
    "\n",
    "# Add Embedding layer\n",
    "model3.add(Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True))\n",
    "\n",
    "# Add Recurrent layer\n",
    "#model.add(Bidirectional(CuDNNGRU(300, return_sequences=True)))\n",
    "model3.add(LSTM(60, return_sequences=True, name='lstm_layer'))\n",
    "model3.add(LSTM(30, return_sequences=True, name='lstm_layer2'))\n",
    "model3.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "model3.add(MaxPooling1D(3))\n",
    "model3.add(GlobalMaxPooling1D())\n",
    "model3.add(BatchNormalization())\n",
    "\n",
    "# Add fully connected layers\n",
    "model3.add(Dense(50, activation='relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Summarize the model\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN GloVe Model 2**\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') <br>\n",
    "embedded_sequences = embedding_layer(sequence_input)<br>\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)<br>\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)<br>\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)<br>\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)<br>\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)<br>\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling<br>\n",
    "l_flat = Flatten()(l_pool3)<br>\n",
    "l_dense = Dense(128, activation='relu')(l_flat)<br>\n",
    "preds = Dense(2, activation='softmax')<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
